# ADULT DATA SET, pre loading and cleaning, renaming.

import numpy as np
import matplotlib.pyplot as plt
import csv
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix, accuracy_score

# Load the dataset into a DataFrame with specified column names
column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',
                'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',
                'hours_per_week', 'native_country', 'income']

# Load the dataset with custom column names and space-separated values
df = pd.read_csv("adult.data", delimiter=',', names=column_names)
# Convert 'sex' column to binary (0 or 1)
df['is_male'] = df['sex'].replace({' Male': 1, ' Female': 0})

# Convert 'race' column to binary (0 or 1) for 'White'
df['is_white'] = df['race'].replace({' White': 1, ' Black': 0})  # Assuming 'White' is the target race

# Convert income column to numeric values
df['income'] = df['income'].apply(lambda x: x.strip())  # Remove any leading/trailing spaces
df['income'] = df['income'].replace(['<=50K', '>50K'], [50000, 75000])  # Map income categories to numeric values

df_filtered_c = df[['capital_gain', 'capital_loss', 'hours_per_week', 'fnlwgt', 'is_male', 'age', 'education_num', 'income']].copy()


# Define the list of columns to keep
columns_to_keep = ['capital_gain', 'capital_loss', 'hours_per_week', 'fnlwgt', 'is_male', 'age', 'education_num']
df_filtered = df.drop(columns=[col for col in df.columns if col not in columns_to_keep])


# Calculate quantile distribution of income for those earning more than $50,000
top_50k_df = df[df['income'] > 50000]
quantile_threshold = top_50k_df['income'].quantile(0.5)  # Calculate 50th percentile

# Create new column 'Y' based on quantile condition
df['Y'] = 0  # Initialize 'Y' column with zeros
df.loc[(df['income'] > 50000) & (df['income'] >= quantile_threshold), 'Y'] = 1  # Set 'Y' to 1 for top 50% earners

# Calculate correlation matrix including 'original_income'
corr_matrix = df_filtered_c.corr()

# Plotting the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()



# Define the list of columns to keep
columns_to_keep = ['capital_gain', 'capital_loss', 'hours_per_week', 'fnlwgt', 'is_male', 'age', 'education_num', 'income']

# Select multiple columns and create a new DataFrame 'df_filtered'
df_filtered = df[columns_to_keep]

# Split the data into features (X) and target variable (Y)
X = df_filtered.drop('income', axis=1)  # Features
Y = df_filtered['income']  # Target variable

# Split the data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Perform clustering to identify clusters based on attributes in the training set
kmeans = KMeans(n_clusters=2, random_state=42)
cluster_labels_train = kmeans.fit_predict(X_train[['capital_gain', 'age', 'education_num']])
cluster_labels_test = kmeans.fit_predict(X_test[['capital_gain', 'age', 'education_num']])

# Add cluster labels as new features to X_train and X_test
X_train['cluster_label'] = cluster_labels_train
X_test['cluster_label'] = cluster_labels_test

# Define the Random Forest classifier
rf = RandomForestClassifier(random_state=42)

# Define the hyperparameter grid for GridSearchCV
param_grid = {
    'n_estimators': [100, 200],  # Number of trees in the forest
    'max_depth': [10, 20],  # Maximum depth of the tree
    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2]  # Minimum number of samples required to be at a leaf node
}

# Create the GridSearchCV object with the Random Forest classifier
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# Extract the best model from the grid search
best_rf = grid_search.best_estimator_

# Make predictions on the test data using the best model
Y_pred = best_rf.predict(X_test)

# Evaluate the best model
accuracy = accuracy_score(Y_test, Y_pred)
precision = precision_score(Y_test, Y_pred, pos_label=50000)  # Use pos_label based on your unique labels

print(f"Best Random Forest Model - Accuracy: {accuracy:.2f}")
print(f"Best Random Forest Model - Precision: {precision:.2f}")
print(f"Best Random Forest Model - Recall: {recall:.2f}")
print(f"Best Random Forest Model - F1-score: {f1:.2f}")

# Plot confusion matrix for the best model
cm = confusion_matrix(Y_test, Y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['<=50K', '>50K'], yticklabels=['<=50K', '>50K'])
plt.title('Confusion Matrix - Best Random Forest Model')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
